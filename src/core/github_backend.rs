use std::path::Path;
use std::sync::mpsc;

use async_llm::{ChatMessage, ChatRequest, Error};
use base64::{Engine, engine::general_purpose};
use image::ImageFormat;
use tokio_stream::StreamExt;

use super::llm_backend::{LLMResponse, LLMBackend, LLMProvider};

/// GitHub Models åç«¯å®ç°
/// æ”¯æŒ GitHub Models API (https://models.inference.ai.azure.com)
#[derive(Clone, Debug)]
pub struct GitHubBackend {
    pub model: String,
    pub api_token: Option<String>,
    pub base_url: String,
}

impl Default for GitHubBackend {
    fn default() -> Self {
        Self {
            model: "gpt-4o".to_string(),
            api_token: std::env::var("GITHUB_TOKEN").ok(),
            base_url: "https://models.inference.ai.azure.com".to_string(),
        }
    }
}

impl GitHubBackend {
    /// åˆ›å»ºæ–°çš„ GitHub åç«¯å®ä¾‹
    pub fn new(model: String) -> Self {
        Self {
            model,
            api_token: std::env::var("GITHUB_TOKEN").ok(),
            base_url: "https://models.inference.ai.azure.com".to_string(),
        }
    }

    /// è®¾ç½® GitHub Token
    pub fn with_api_key(mut self, api_key: String) -> Self {
        self.api_token = Some(api_key);
        self
    }

    /// è®¾ç½®è‡ªå®šä¹‰ API ç«¯ç‚¹
     #[allow(dead_code)]
    pub fn with_base_url(mut self, base_url: String) -> Self {
        self.base_url = base_url;
        self
    }

    /// ä»å›¾ç‰‡è·¯å¾„ç”Ÿæˆ base64 ç¼–ç 
    fn image_to_base64(&self, path: &Path) -> Result<String, Box<dyn std::error::Error>> {
        let image = image::ImageReader::open(path)?.decode()?;
        let mut buf = Vec::new();
        image.write_to(&mut std::io::Cursor::new(&mut buf), ImageFormat::Png)?;
        Ok(general_purpose::STANDARD.encode(&buf))
    }

    /// æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    fn build_messages(&self, text: &str, image_path: Option<&Path>) -> Vec<ChatMessage> {
        if let Some(path) = image_path {
            tracing::debug!("[github_backend] Converting image to base64: {}", path.display());
            match self.image_to_base64(path) {
                Ok(base64) => {
                    tracing::debug!("[github_backend] Image converted to base64 successfully");
                    // GitHub Models API éœ€è¦ data URL æ ¼å¼: data:image/png;base64,<base64_string>
                    let data_url = format!("data:image/png;base64,{}", base64);
                    vec![
                        ChatMessage::system("You are GitHub Copilot, a helpful AI assistant for analyzing questions and images."),
                        ChatMessage::user_image_with_text(text, data_url.as_str()),
                    ]
                }
                Err(e) => {
                    tracing::error!("[github_backend] Failed to convert image to base64: {}", e);
                    vec![
                        ChatMessage::system("you have to follow the follow rules"),
                        ChatMessage::user(text),
                    ]
                }
            }
        } else {
            tracing::debug!("[github_backend] Text-only request");
            tracing::info!("messages: {:?}", text);
            vec![
                ChatMessage::system("you have to follow the follow rules"),
                ChatMessage::user(text),
            ]
        }
    }

    /// è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨ GitHub Models API
    fn setup_environment(&self) {
        if let Some(api_token) = &self.api_token {
            unsafe {
                std::env::set_var("GITHUB_TOKEN", api_token);
            }
        } else {
            tracing::error!("[github_backend] No GitHub token available. Please set GITHUB_TOKEN environment variable or use with_api_key()");
        }
    }

    /// å°è¯•æµå¼è¯·æ±‚
    async fn try_streaming_request(
        &self,
        messages: Vec<ChatMessage>,
        response_sender: &mpsc::Sender<LLMResponse>,
    ) -> Result<String, Error> {
        tracing::info!("[github_backend] Attempting streaming request to GitHub Models...");
        
        // ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡
        self.setup_environment();
        
        let stream_request = ChatRequest::new(&self.model, messages).with_stream();
        
        let mut response = stream_request.send_stream().await?;
        tracing::info!("[github_backend] Streaming request successful, processing response...");
        
        let mut accumulated_content = String::new();

        while let Some(result) = response.next().await {
            match result {
                Ok(response) => {
                    if let Some(choice) = response.choices.first() {
                        if let Some(delta) = &choice.delta {
                            if let Some(content) = &delta.content {
                                accumulated_content.push_str(content);
                                
                                tracing::trace!("[github_backend] Streaming response chunk, total length: {}", accumulated_content.len());
                                let _ = response_sender.send(LLMResponse {
                                    content: accumulated_content.clone(),
                                    is_complete: false,
                                });
                            }
                        }
                    }
                }
                Err(e) => {
                    tracing::warn!("[github_backend] GitHub streaming error during processing: {}", e);
                    return Err(e);
                }
            }
        }

        tracing::info!("[github_backend] GitHub streaming response completed, total length: {}", accumulated_content.len());
        Ok(accumulated_content)
    }

    /// å°è¯•éæµå¼è¯·æ±‚
    async fn try_non_streaming_request(
        &self,
        messages: Vec<ChatMessage>,
    ) -> Result<String, Error> {
        tracing::info!("[github_backend] Attempting non-streaming request to GitHub Models...");
        
        // ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡
        self.setup_environment();
        
        let request = ChatRequest::new(&self.model, messages);
        
        let response = request.send().await?;
        tracing::info!("[github_backend] Non-streaming request successful");
        
        let content = if let Some(choice) = response.choices.first() {
            if let Some(message) = &choice.message {
                message.content.clone().unwrap_or_default()
            } else {
                String::new()
            }
        } else {
            String::new()
        };
        
        tracing::info!("[github_backend] GitHub non-streaming response completed, length: {}", content.len());
        Ok(content)
    }
}

#[async_trait::async_trait]
impl LLMBackend for GitHubBackend {
    fn provider(&self) -> LLMProvider {
        LLMProvider::GitHub
    }

    fn model_name(&self) -> &str {
        &self.model
    }

    async fn send_message(
        &self,
        text: String,
        image_path: Option<&Path>,
        response_sender: mpsc::Sender<LLMResponse>,
    ) -> Result<(), Error> {
        tracing::info!("[github_backend] Sending message to GitHub Models API...");
        
        if self.api_token.is_none() {
            let error_msg = "GitHub token not available. Please set GITHUB_TOKEN environment variable.".to_string();
            tracing::error!("[github_backend] {}", error_msg);
            let _ = response_sender.send(LLMResponse {
                content: format!("Error: {}", error_msg),
                is_complete: true,
            });
            return Err(Error::Stream(error_msg.into()));
        }

        let messages = self.build_messages(&text, image_path);

        // é¦–å…ˆå°è¯•æµå¼è¯·æ±‚
        match self.try_streaming_request(messages.clone(), &response_sender).await {
            Ok(content) => {
                // æµå¼è¯·æ±‚æˆåŠŸå®Œæˆ
                let _ = response_sender.send(LLMResponse {
                    content,
                    is_complete: true,
                });
                Ok(())
            }
            Err(e) => {
                // æµå¼è¯·æ±‚å¤±è´¥ï¼Œå°è¯•éæµå¼è¯·æ±‚
                tracing::warn!("[github_backend] Streaming request failed: {}, trying non-streaming request...", e);
                
                match self.try_non_streaming_request(messages).await {
                    Ok(content) => {
                        // å‘é€å®Œæ•´å“åº”
                        let _ = response_sender.send(LLMResponse {
                            content,
                            is_complete: true,
                        });
                        Ok(())
                    }
                    Err(e2) => {
                        tracing::error!("[github_backend] Both streaming and non-streaming requests failed. Streaming error: {}, Non-streaming error: {}", e, e2);
                        let _ = response_sender.send(LLMResponse {
                            content: format!("Error: Both streaming and non-streaming requests failed. Last error: {}", e2),
                            is_complete: true,
                        });
                        Err(e2)
                    }
                }
            }
        }
    }

    async fn test_availability(&self) -> Result<String, Error> {
        tracing::info!("[github_backend] Testing GitHub Models API availability...");
        
        if self.api_token.is_none() {
            let error_msg = "GitHub token not available. Please set GITHUB_TOKEN environment variable.";
            tracing::error!("[github_backend] {}", error_msg);
            return Err(Error::Stream(error_msg.into()));
        }
        
        let messages = vec![
            ChatMessage::system("You are GitHub Copilot, a helpful AI assistant."),
            ChatMessage::user("Please respond with 'Hello from GitHub Copilot!' to confirm you are available."),
        ];

        // ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡
        self.setup_environment();

        // é¦–å…ˆå°è¯•æµå¼è¯·æ±‚
        tracing::info!("[github_backend] Attempting streaming test request...");
        let stream_request = ChatRequest::new(&self.model, messages.clone()).with_stream();

        match stream_request.send_stream().await {
            Ok(mut response) => {
                let mut accumulated_content = String::new();

                while let Some(result) = response.next().await {
                    match result {
                        Ok(response) => {
                            if let Some(choice) = response.choices.first() {
                                if let Some(delta) = &choice.delta {
                                    if let Some(content) = &delta.content {
                                        accumulated_content.push_str(content);
                                    }
                                }
                            }
                        }
                        Err(e) => {
                            tracing::error!("[github_backend] GitHub streaming test error: {}", e);
                            return Err(e);
                        }
                    }
                }

                if !accumulated_content.is_empty() {
                    tracing::info!("[github_backend] GitHub streaming test successful: {}", accumulated_content);
                    Ok(accumulated_content)
                } else {
                    tracing::error!("[github_backend] GitHub streaming test failed: No response content");
                    Err(Error::Stream("No response content from GitHub Models".into()))
                }
            }
            Err(e) => {
                // æµå¼è¯·æ±‚å¤±è´¥ï¼Œå°è¯•éæµå¼è¯·æ±‚
                tracing::warn!("[github_backend] Streaming test failed: {}, trying non-streaming test...", e);
                
                let non_stream_request = ChatRequest::new(&self.model, messages);
                
                match non_stream_request.send().await {
                    Ok(response) => {
                        let content = if let Some(choice) = response.choices.first() {
                            if let Some(message) = &choice.message {
                                message.content.clone().unwrap_or_default()
                            } else {
                                String::new()
                            }
                        } else {
                            String::new()
                        };
                        
                        if !content.is_empty() {
                            tracing::info!("[github_backend] GitHub non-streaming test successful: {}", content);
                            Ok(content)
                        } else {
                            tracing::error!("[github_backend] GitHub non-streaming test failed: No response content");
                            Err(Error::Stream("No response content from GitHub Models".into()))
                        }
                    }
                    Err(e2) => {
                        tracing::error!("[github_backend] Both streaming and non-streaming tests failed. Streaming error: {}, Non-streaming error: {}", e, e2);
                        Err(e2)
                    }
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn setup_test_environment() {
        dotenvy::dotenv().ok();
        
        println!("ğŸ”§ GitHub æµ‹è¯•ç¯å¢ƒé…ç½®:");
        if let Ok(_) = std::env::var("GITHUB_TOKEN") {
            println!("   GitHub Token: [å·²é…ç½®]");
        } else {
            println!("   GitHub Token: [æœªé…ç½®] - å°†è·³è¿‡çœŸå® API æµ‹è¯•");
        }
    }

    #[tokio::test]
    async fn test_github_backend_creation() {
        let _ = tracing_subscriber::fmt::try_init();
        
        // æµ‹è¯•é»˜è®¤åˆ›å»º
        let backend = GitHubBackend::default();
        assert_eq!(backend.model, "gpt-4o");
        assert_eq!(backend.base_url, "https://models.inference.ai.azure.com");
        assert_eq!(backend.provider(), LLMProvider::GitHub);
        assert_eq!(backend.model_name(), "gpt-4o");

        // æµ‹è¯•è‡ªå®šä¹‰åˆ›å»º
        let custom_backend = GitHubBackend::new("gpt-3.5-turbo".to_string())
            .with_api_key("test_token".to_string())
            .with_base_url("https://custom.api.com".to_string());
        
        assert_eq!(custom_backend.model, "gpt-3.5-turbo");
        assert_eq!(custom_backend.api_token, Some("test_token".to_string()));
        assert_eq!(custom_backend.base_url, "https://custom.api.com");
        
        println!("âœ… GitHub backend creation tests passed!");
    }

    #[tokio::test]
    async fn test_github_backend_availability() {
        setup_test_environment();
        let _ = tracing_subscriber::fmt::try_init();
        
        let backend = GitHubBackend::default();
        
        match backend.test_availability().await {
            Ok(response) => {
                println!("âœ… GitHub Models API å¯ç”¨! å“åº”: {}", response);
                assert!(!response.is_empty(), "GitHub response should not be empty");
            }
            Err(e) => {
                println!("â„¹ï¸ GitHub Models API æµ‹è¯•å¤±è´¥ (å¯èƒ½å› ä¸ºæ²¡æœ‰é…ç½® GITHUB_TOKEN): {}", e);
                // åœ¨æ²¡æœ‰ token çš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯é¢„æœŸçš„è¡Œä¸º
                eprintln!("GitHub test failed (this might be expected if no GITHUB_TOKEN is configured): {}", e);
            }
        }
    }

    #[tokio::test]
    async fn test_github_backend_send_message() {
        setup_test_environment();
        let _ = tracing_subscriber::fmt::try_init();
        
        let backend = GitHubBackend::default();
        let (sender, receiver) = mpsc::channel();
        
        let test_message = "Hello GitHub Copilot! Please respond briefly.".to_string();
        
        // å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å‘é€æ¶ˆæ¯
        let send_task = tokio::spawn(async move {
            backend.send_message(test_message, None, sender).await
        });
        
        // æ”¶é›†å“åº”
        let mut responses = Vec::new();
        let mut final_content = String::new();
        
        // è®¾ç½®è¶…æ—¶ä»¥é¿å…æµ‹è¯•æ— é™ç­‰å¾…
        let timeout_duration = std::time::Duration::from_secs(30);
        let start_time = std::time::Instant::now();
        
        while start_time.elapsed() < timeout_duration {
            match receiver.try_recv() {
                Ok(response) => {
                    responses.push(response.clone());
                    final_content = response.content.clone();
                    
                    if response.is_complete {
                        break;
                    }
                }
                Err(mpsc::TryRecvError::Empty) => {
                    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                }
                Err(mpsc::TryRecvError::Disconnected) => {
                    break;
                }
            }
        }
        
        // ç­‰å¾…å‘é€ä»»åŠ¡å®Œæˆ
        match send_task.await {
            Ok(Ok(())) => {
                println!("âœ… GitHub æ¶ˆæ¯å‘é€æˆåŠŸ!");
                println!("ğŸ“ æœ€ç»ˆå“åº”é•¿åº¦: {}", final_content.len());
                println!("ğŸ“Š æ€»å…±æ”¶åˆ° {} ä¸ªå“åº”ç‰‡æ®µ", responses.len());
                
                if !final_content.is_empty() && !final_content.starts_with("Error:") {
                    println!("ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {}...", 
                        final_content.chars().take(100).collect::<String>());
                }
            }
            Ok(Err(e)) => {
                println!("â„¹ï¸ GitHub è¯·æ±‚å¤±è´¥ (å¯èƒ½å› ä¸ºæ²¡æœ‰é…ç½® GITHUB_TOKEN): {}", e);
                eprintln!("GitHub send message test failed (this might be expected if no GITHUB_TOKEN is configured): {}", e);
            }
            Err(e) => {
                println!("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e);
            }
        }
    }

    #[tokio::test]
    async fn test_github_backend_without_token() {
        let _ = tracing_subscriber::fmt::try_init();
        
        // åˆ›å»ºæ²¡æœ‰ token çš„åç«¯
        let backend = GitHubBackend::new("gpt-4o".to_string()); // ä¸è®¾ç½® token
        
        // æµ‹è¯•åº”è¯¥å¤±è´¥å¹¶è¿”å›é”™è¯¯
        match backend.test_availability().await {
            Ok(_) => {
                // å¦‚æœç¯å¢ƒå˜é‡ä¸­æœ‰ GITHUB_TOKENï¼Œè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¼šæˆåŠŸ
                println!("â„¹ï¸ æµ‹è¯•æˆåŠŸï¼Œå¯èƒ½æ˜¯å› ä¸ºç¯å¢ƒå˜é‡ä¸­æœ‰ GITHUB_TOKEN");
            }
            Err(e) => {
                println!("âœ… æ­£ç¡®å¤„ç†äº†ç¼ºå°‘ token çš„æƒ…å†µ: {}", e);
                assert!(e.to_string().contains("GitHub token not available"));
            }
        }
    }

    #[tokio::test]
    async fn test_github_backend_send_message_with_image() {
        setup_test_environment();
        let _ = tracing_subscriber::fmt::try_init();
        
        let backend = GitHubBackend::default();
        let (sender, receiver) = mpsc::channel();
        
        // ä½¿ç”¨é¡¹ç›®ä¸­çš„å›¾æ ‡ä½œä¸ºæµ‹è¯•å›¾ç‰‡
        let image_path = Path::new("icon/icon.png");
        
        // æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if !image_path.exists() {
            println!("âš ï¸ æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡å›¾ç‰‡æµ‹è¯•: {}", image_path.display());
            return;
        }
        
        let test_message = "Please describe what you see in this image briefly.".to_string();
        
        println!("ğŸ“¸ å‘é€å¸¦å›¾ç‰‡çš„æ¶ˆæ¯æµ‹è¯•ï¼Œå›¾ç‰‡è·¯å¾„: {}", image_path.display());
        
        // å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å‘é€æ¶ˆæ¯ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
        let send_task = tokio::spawn(async move {
            backend.send_message(test_message, Some(image_path), sender).await
        });
        
        // æ”¶é›†å“åº”
        let mut responses = Vec::new();
        let mut final_content = String::new();
        
        // è®¾ç½®è¶…æ—¶ä»¥é¿å…æµ‹è¯•æ— é™ç­‰å¾…
        let timeout_duration = std::time::Duration::from_secs(30);
        let start_time = std::time::Instant::now();
        
        while start_time.elapsed() < timeout_duration {
            match receiver.try_recv() {
                Ok(response) => {
                    responses.push(response.clone());
                    final_content = response.content.clone();
                    
                    if response.is_complete {
                        break;
                    }
                }
                Err(mpsc::TryRecvError::Empty) => {
                    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                }
                Err(mpsc::TryRecvError::Disconnected) => {
                    break;
                }
            }
        }
        
        // ç­‰å¾…å‘é€ä»»åŠ¡å®Œæˆ
        match send_task.await {
            Ok(Ok(())) => {
                println!("âœ… GitHub å›¾ç‰‡æ¶ˆæ¯å‘é€æˆåŠŸ!");
                println!("ğŸ“ æœ€ç»ˆå“åº”é•¿åº¦: {}", final_content.len());
                println!("ğŸ“Š æ€»å…±æ”¶åˆ° {} ä¸ªå“åº”ç‰‡æ®µ", responses.len());
                
                if !final_content.is_empty() && !final_content.starts_with("Error:") {
                    println!("ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {}...", 
                        final_content.chars().take(150).collect::<String>());
                }
            }
            Ok(Err(e)) => {
                println!("â„¹ï¸ GitHub å›¾ç‰‡è¯·æ±‚å¤±è´¥ (å¯èƒ½å› ä¸ºæ²¡æœ‰é…ç½® GITHUB_TOKEN): {}", e);
                eprintln!("GitHub send message with image test failed (this might be expected if no GITHUB_TOKEN is configured): {}", e);
            }
            Err(e) => {
                println!("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e);
            }
        }
    }
}