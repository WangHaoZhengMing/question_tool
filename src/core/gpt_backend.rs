use std::path::Path;
use std::sync::mpsc;

use async_llm::{ChatMessage, ChatRequest, Error};
use base64::{Engine, engine::general_purpose};
use image::ImageFormat;
use tokio_stream::StreamExt;

use super::llm_backend::{LLMBackend, LLMProvider, LLMResponse};

/// GPT åç«¯å®ç°
#[derive(Clone, Debug)]
pub struct GPTBackend {
    pub model: String,
    pub api_key: Option<String>,
    pub base_url: Option<String>,
}

impl Default for GPTBackend {
    fn default() -> Self {
        Self {
            model: "gpt-4o".to_string(),
            api_key: None,
            base_url: None,
        }
    }
}

impl GPTBackend {
    #[allow(dead_code)]
    pub fn new(model: String) -> Self {
        Self {
            model,
            api_key: None,
            base_url: None,
        }
    }

    pub fn with_api_key(mut self, api_key: String) -> Self {
        self.api_key = Some(api_key);
        self
    }

    pub fn with_base_url(mut self, base_url: String) -> Self {
        self.base_url = Some(base_url);
        self
    }

    /// ä»å›¾ç‰‡è·¯å¾„ç”Ÿæˆ base64 ç¼–ç 
    fn image_to_base64(&self, path: &Path) -> Result<String, Box<dyn std::error::Error>> {
        // æ‰“å¼€å¹¶è§£ç å›¾ç‰‡ï¼Œç„¶åç¼–ç ä¸º PNG æ ¼å¼å­—èŠ‚æµï¼Œæœ€åè½¬ä¸º base64 å­—ç¬¦ä¸²
        let image = image::ImageReader::open(path)?.decode()?;
        let mut buf = Vec::new();
        image.write_to(&mut std::io::Cursor::new(&mut buf), ImageFormat::Png)?;
        Ok(general_purpose::STANDARD.encode(&buf))
    }

    /// æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    fn build_messages(&self, text: &str, image_path: Option<&Path>) -> Vec<ChatMessage> {
        if let Some(path) = image_path {
            // å¦‚æœæœ‰å›¾ç‰‡ï¼Œè½¬æ¢ä¸º base64
            tracing::info!(
                "[gpt_backend] Converting image to base64: {}",
                path.display()
            );
            match self.image_to_base64(path) {
                Ok(base64) => {
                    tracing::info!("[gpt_backend] Image converted to base64 successfully");
                    let data_url = format!("data:image/png;base64,{}", base64);
                    vec![
                        ChatMessage::system(""),
                        ChatMessage::user_image_with_text(text, data_url.as_str()),
                    ]
                }
                Err(e) => {
                    tracing::error!("[gpt_backend] Failed to convert image to base64: {}", e);
                    vec![
                        ChatMessage::system(
                            "You are a helpful assistant for analyzing questions and images.",
                        ),
                        ChatMessage::user(text),
                    ]
                }
            }
        } else {
            // åªæœ‰æ–‡æœ¬
            tracing::info!("[gpt_backend] Text-only request");
            vec![
                ChatMessage::system(
                    "",
                ),
                ChatMessage::user(text),
            ]
        }
    }

    /// å°è¯•æµå¼è¯·æ±‚
    async fn try_streaming_request(
        &self,
        messages: Vec<ChatMessage>,
        response_sender: &mpsc::Sender<LLMResponse>,
    ) -> Result<String, Error> {
        tracing::info!("[gpt_backend] Attempting streaming request to GPT...");
        let stream_request = ChatRequest::new(&self.model, messages).with_stream();

        let mut response = stream_request.send_stream().await?;
        tracing::info!("[gpt_backend] Send streaming request successful, processing response...");

        let mut accumulated_content = String::new();

        while let Some(result) = response.next().await {
            match result {
                Ok(response) => {
                    if let Some(choice) = response.choices.first() {
                        if let Some(delta) = &choice.delta {
                            if let Some(content) = &delta.content {
                                accumulated_content.push_str(content);

                                // å‘é€æµå¼æ›´æ–°
                                tracing::trace!(
                                    "[gpt_backend] Streaming response chunk, total length: {}",
                                    accumulated_content.len()
                                );
                                let _ = response_sender.send(LLMResponse {
                                    content: accumulated_content.clone(),
                                    is_complete: false,
                                });
                            }
                        }
                    }
                }
                Err(e) => {
                    tracing::warn!("[gpt_backend] GPT streaming error during processing: {}", e);
                    return Err(e);
                }
            }
        }

        tracing::info!(
            "[gpt_backend] GPT streaming response completed, total length: {}",
            accumulated_content.len()
        );
        Ok(accumulated_content)
    }

    /// å°è¯•éæµå¼è¯·æ±‚
    async fn try_non_streaming_request(&self, messages: Vec<ChatMessage>) -> Result<String, Error> {
        tracing::info!("[gpt_backend] Attempting non-streaming request to GPT...");
        let request = ChatRequest::new(&self.model, messages);

        let response = request.send().await?;
        tracing::info!("[gpt_backend] Non-streaming request successful");

        let content = if let Some(choice) = response.choices.first() {
            if let Some(message) = &choice.message {
                message.content.clone().unwrap_or_default()
            } else {
                String::new()
            }
        } else {
            String::new()
        };

        tracing::info!(
            "[gpt_backend] GPT non-streaming response completed, length: {}",
            content.len()
        );
        Ok(content)
    }
}

#[async_trait::async_trait]
impl LLMBackend for GPTBackend {
    fn provider(&self) -> LLMProvider {
        LLMProvider::GPT
    }

    fn model_name(&self) -> &str {
        &self.model
    }

    async fn send_message(
        &self,
        text: String,
        image_path: Option<&Path>,
        response_sender: mpsc::Sender<LLMResponse>,
    ) -> Result<(), Error> {
        let messages = self.build_messages(&text, image_path);

        // é¦–å…ˆå°è¯•æµå¼è¯·æ±‚
        match self
            .try_streaming_request(messages.clone(), &response_sender)
            .await
        {
            Ok(content) => {
                // æµå¼è¯·æ±‚æˆåŠŸå®Œæˆ
                let _ = response_sender.send(LLMResponse {
                    content,
                    is_complete: true,
                });
                Ok(())
            }
            Err(e) => {
                // æµå¼è¯·æ±‚å¤±è´¥ï¼Œå°è¯•éæµå¼è¯·æ±‚
                tracing::warn!(
                    "[gpt_backend] Streaming request failed: {}, trying non-streaming request...",
                    e
                );

                match self.try_non_streaming_request(messages).await {
                    Ok(content) => {
                        // å‘é€å®Œæ•´å“åº”
                        let _ = response_sender.send(LLMResponse {
                            content,
                            is_complete: true,
                        });
                        Ok(())
                    }
                    Err(e2) => {
                        tracing::error!(
                            "[gpt_backend] Both streaming and non-streaming requests failed. Streaming error: {}, Non-streaming error: {}",
                            e,
                            e2
                        );
                        let _ = response_sender.send(LLMResponse {
                            content: format!("Error: Both streaming and non-streaming requests failed. Last error: {}", e2),
                            is_complete: true,
                        });
                        Err(e2)
                    }
                }
            }
        }
    }

    async fn test_availability(&self) -> Result<String, Error> {
        tracing::info!("[gpt_backend] Testing GPT availability...");

        let messages = vec![
            ChatMessage::system("You are a helpful assistant."),
            ChatMessage::user(
                "Please respond with 'Hello! I am working correctly.' to confirm you are available.",
            ),
        ];

        // é¦–å…ˆå°è¯•æµå¼è¯·æ±‚
        tracing::info!("[gpt_backend] Attempting streaming test request...");
        let stream_request = ChatRequest::new(&self.model, messages.clone()).with_stream();

        match stream_request.send_stream().await {
            Ok(mut response) => {
                let mut accumulated_content = String::new();

                while let Some(result) = response.next().await {
                    match result {
                        Ok(response) => {
                            if let Some(choice) = response.choices.first() {
                                if let Some(delta) = &choice.delta {
                                    if let Some(content) = &delta.content {
                                        accumulated_content.push_str(content);
                                    }
                                }
                            }
                        }
                        Err(e) => {
                            tracing::error!("[gpt_backend] GPT streaming test error: {}", e);
                            return Err(e);
                        }
                    }
                }

                if !accumulated_content.is_empty() {
                    tracing::info!(
                        "[gpt_backend] GPT streaming test successful: {}",
                        accumulated_content
                    );
                    Ok(accumulated_content)
                } else {
                    tracing::error!("[gpt_backend] GPT streaming test failed: No response content");
                    Err(Error::Stream("No response content from GPT".into()))
                }
            }
            Err(e) => {
                // æµå¼è¯·æ±‚å¤±è´¥ï¼Œå°è¯•éæµå¼è¯·æ±‚
                tracing::warn!(
                    "[gpt_backend] Streaming test failed: {}, trying non-streaming test...",
                    e
                );

                let non_stream_request = ChatRequest::new(&self.model, messages);

                match non_stream_request.send().await {
                    Ok(response) => {
                        let content = if let Some(choice) = response.choices.first() {
                            if let Some(message) = &choice.message {
                                message.content.clone().unwrap_or_default()
                            } else {
                                String::new()
                            }
                        } else {
                            String::new()
                        };

                        if !content.is_empty() {
                            tracing::info!(
                                "[gpt_backend] GPT non-streaming test successful: {}",
                                content
                            );
                            Ok(content)
                        } else {
                            tracing::error!(
                                "[gpt_backend] GPT non-streaming test failed: No response content"
                            );
                            Err(Error::Stream("No response content from GPT".into()))
                        }
                    }
                    Err(e2) => {
                        tracing::error!(
                            "[gpt_backend] Both streaming and non-streaming tests failed. Streaming error: {}, Non-streaming error: {}",
                            e,
                            e2
                        );
                        Err(e2)
                    }
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn setup_test_environment() {
        dotenvy::dotenv().ok();

        // è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨ä½ çš„è‡ªå®šä¹‰ API ç«¯ç‚¹
        unsafe {
            if let (Ok(api_key), Ok(base_url)) = (
                std::env::var("OPENROUTER_API_KEY"),
                std::env::var("OPENROUTER_BASE_URL"),
            ) {
                std::env::set_var("OPENAI_API_KEY", api_key);
                std::env::set_var("OPENAI_BASE_URL", base_url);
            } else if let (Ok(api_key), Ok(base_url)) = (
                std::env::var("OPENAI_API_KEY"),
                std::env::var("OPENAI_BASE_URL"),
            ) {
                // å¦‚æœç›´æ¥é…ç½®äº† OPENAI_* å˜é‡ï¼Œåˆ™ä½¿ç”¨å®ƒä»¬
                std::env::set_var("OPENAI_API_KEY", api_key);
                std::env::set_var("OPENAI_BASE_URL", base_url);
            }
        }

        println!("ğŸ”§ æµ‹è¯•ç¯å¢ƒé…ç½®:");
        if let Ok(base_url) = std::env::var("OPENAI_BASE_URL") {
            println!("   Base URL: {}", base_url);
        }
        if let Ok(_) = std::env::var("OPENAI_API_KEY") {
            println!("   API Key: [å·²é…ç½®]");
        }
    }

    #[tokio::test]
    async fn test_gpt_connection() {
        // åˆå§‹åŒ–ç¯å¢ƒå˜é‡å’Œæ—¥å¿—
        setup_test_environment();
        let _ = tracing_subscriber::fmt::try_init();

        let backend = GPTBackend::default();

        match backend.test_availability().await {
            Ok(response) => {
                println!("âœ… GPT å¯ç”¨! å“åº”: {}", response);
                assert!(!response.is_empty(), "GPT response should not be empty");
            }
            Err(e) => {
                println!("âŒ GPT ä¸å¯ç”¨: {}", e);
                eprintln!(
                    "GPT test failed (this might be expected if no API key is configured): {}",
                    e
                );
            }
        }
    }

    #[tokio::test]
    async fn test_send_message_to_gpt() {
        setup_test_environment();
        let _ = tracing_subscriber::fmt::try_init();

        let backend = GPTBackend::default();
        let (sender, receiver) = mpsc::channel();

        let test_message = "Hello, this is a test message.".to_string();

        // å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å‘é€æ¶ˆæ¯
        let send_task =
            tokio::spawn(async move { backend.send_message(test_message, None, sender).await });

        // æ”¶é›†å“åº”
        let mut responses = Vec::new();
        let mut final_content = String::new();

        // è®¾ç½®è¶…æ—¶ä»¥é¿å…æµ‹è¯•æ— é™ç­‰å¾…
        let timeout_duration = std::time::Duration::from_secs(30);
        let start_time = std::time::Instant::now();

        while start_time.elapsed() < timeout_duration {
            match receiver.try_recv() {
                Ok(response) => {
                    responses.push(response.clone());
                    final_content = response.content.clone();

                    if response.is_complete {
                        break;
                    }
                }
                Err(mpsc::TryRecvError::Empty) => {
                    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                }
                Err(mpsc::TryRecvError::Disconnected) => {
                    break;
                }
            }
        }

        // ç­‰å¾…å‘é€ä»»åŠ¡å®Œæˆ
        match send_task.await {
            Ok(Ok(())) => {
                println!("âœ… æ¶ˆæ¯å‘é€æˆåŠŸ!");
                println!("ğŸ“ æœ€ç»ˆå“åº”é•¿åº¦: {}", final_content.len());
                println!("ğŸ“Š æ€»å…±æ”¶åˆ° {} ä¸ªå“åº”ç‰‡æ®µ", responses.len());

                if !final_content.is_empty() {
                    println!(
                        "ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {}...",
                        final_content.chars().take(100).collect::<String>()
                    );
                }
            }
            Ok(Err(e)) => {
                println!("âŒ GPT è¯·æ±‚å¤±è´¥: {}", e);
                eprintln!(
                    "Send message test failed (this might be expected if no API key is configured): {}",
                    e
                );
            }
            Err(e) => {
                println!("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e);
            }
        }
    }
}
