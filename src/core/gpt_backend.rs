use std::path::Path;
use std::sync::mpsc;

use async_llm::{ChatMessage, ChatRequest, Error};
use base64::{Engine, engine::general_purpose};
use image::ImageFormat;
use tokio_stream::StreamExt;

use super::llm_backend::{LLMBackend, LLMProvider, LLMResponse};

/// GPT åç«¯å®ç°
#[derive(Clone, Debug)]
pub struct GPTBackend {
    pub model: String,
    pub api_key: Option<String>,
    pub base_url: Option<String>,
}

impl Default for GPTBackend {
    fn default() -> Self {
        Self {
            model: "gpt-4o".to_string(),
            api_key: None,
            base_url: Some("https://api.tu-zi.com/v1".to_string()),
        }
    }
}

impl GPTBackend {
    #[allow(dead_code)]
    pub fn new(model: String) -> Self {
        Self {
            model,
            api_key: None,
            base_url: None,
        }
    }

    pub fn with_api_key(mut self, api_key: String) -> Self {
        self.api_key = Some(api_key);
        self
    }

    pub fn with_base_url(mut self, base_url: String) -> Self {
        self.base_url = Some(base_url);
        self
    }

    /// ä»å›¾ç‰‡è·¯å¾„ç”Ÿæˆ base64 ç¼–ç 
    fn image_to_base64(&self, path: &Path) -> Result<String, Box<dyn std::error::Error>> {
        // æ‰“å¼€å¹¶è§£ç å›¾ç‰‡ï¼Œç„¶åç¼–ç ä¸º PNG æ ¼å¼å­—èŠ‚æµï¼Œæœ€åè½¬ä¸º base64 å­—ç¬¦ä¸²
        let image = image::ImageReader::open(path)?.decode()?;
        let mut buf = Vec::new();
        image.write_to(&mut std::io::Cursor::new(&mut buf), ImageFormat::Png)?;
        Ok(general_purpose::STANDARD.encode(&buf))
    }

    /// æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    fn build_messages(&self, text: &str, image_path: Option<&Path>) -> Vec<ChatMessage> {
        if let Some(path) = image_path {
            // å¦‚æœæœ‰å›¾ç‰‡ï¼Œè½¬æ¢ä¸º base64
            tracing::info!(
                "[gpt_backend] Converting image to base64: {}",
                path.display()
            );
            match self.image_to_base64(path) {
                Ok(base64) => {
                    tracing::info!("[gpt_backend] Image converted to base64 successfully");
                    let data_url = format!("data:image/png;base64,{}", base64);
                    vec![
                        ChatMessage::system(""),
                        ChatMessage::user_image_with_text(text, data_url.as_str()),
                    ]
                }
                Err(e) => {
                    tracing::error!("[gpt_backend] Failed to convert image to base64: {}", e);
                    vec![
                        ChatMessage::system(
                            "You are a helpful assistant for analyzing questions and images.",
                        ),
                        ChatMessage::user(text),
                    ]
                }
            }
        } else {
            // åªæœ‰æ–‡æœ¬
            tracing::info!("[gpt_backend] Text-only request");
            tracing::info!("messages: {:?}", text);
            vec![ChatMessage::system(""), ChatMessage::user(text)]
        }

    }

    /// è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„ API key å’Œ base URL
    fn setup_environment(&self) {
        if let Some(api_key) = &self.api_key {
            unsafe {
                std::env::set_var("OPENAI_API_KEY", api_key);
                tracing::debug!("[gpt_backend] Set OPENAI_API_KEY environment variable");
            }
        }

        if let Some(base_url) = &self.base_url {
            unsafe {
                std::env::set_var("OPENAI_BASE_URL", base_url);
                tracing::debug!("[gpt_backend] Set OPENAI_BASE_URL to: {}", base_url);
            }
        }
    }

    /// å°è¯•æµå¼è¯·æ±‚
    async fn try_streaming_request(
        &self,
        messages: Vec<ChatMessage>,
        response_sender: &mpsc::Sender<LLMResponse>,
    ) -> Result<String, Error> {
        tracing::info!("[gpt_backend] Attempting streaming request to GPT...");

        // è®¾ç½®ç¯å¢ƒå˜é‡
        self.setup_environment();

        let stream_request = ChatRequest::new(&self.model, messages).with_stream();

        let mut response = stream_request.send_stream().await?;
        tracing::info!("[gpt_backend] Send streaming request successful, processing response...");

        let mut accumulated_content = String::new();

        while let Some(result) = response.next().await {
            match result {
                Ok(response) => {
                    if let Some(choice) = response.choices.first() {
                        if let Some(delta) = &choice.delta {
                            if let Some(content) = &delta.content {
                                accumulated_content.push_str(content);

                                // å‘é€æµå¼æ›´æ–°
                                tracing::trace!(
                                    "[gpt_backend] Streaming response chunk, total length: {}",
                                    accumulated_content.len()
                                );
                                let _ = response_sender.send(LLMResponse {
                                    content: accumulated_content.clone(),
                                    is_complete: false,
                                });
                            }
                        }
                    }
                }
                Err(e) => {
                    tracing::warn!("[gpt_backend] GPT streaming error during processing: {}", e);
                    return Err(e);
                }
            }
        }

        tracing::info!(
            "[gpt_backend] GPT streaming response completed, total length: {}",
            accumulated_content.len()
        );
        Ok(accumulated_content)
    }

    /// å°è¯•éæµå¼è¯·æ±‚
    async fn try_non_streaming_request(&self, messages: Vec<ChatMessage>) -> Result<String, Error> {
        tracing::info!("[gpt_backend] Attempting non-streaming request to GPT...");

        // è®¾ç½®ç¯å¢ƒå˜é‡
        self.setup_environment();

        let request = ChatRequest::new(&self.model, messages);

        let response = request.send().await?;
        tracing::info!("[gpt_backend] Non-streaming request successful");

        let content = if let Some(choice) = response.choices.first() {
            if let Some(message) = &choice.message {
                message.content.clone().unwrap_or_default()
            } else {
                String::new()
            }
        } else {
            String::new()
        };

        tracing::info!(
            "[gpt_backend] GPT non-streaming response completed, length: {}",
            content.len()
        );
        Ok(content)
    }
}

#[async_trait::async_trait]
impl LLMBackend for GPTBackend {
    fn provider(&self) -> LLMProvider {
        LLMProvider::GPT
    }

    fn model_name(&self) -> &str {
        &self.model
    }

    async fn send_message(
        &self,
        text: String,
        image_path: Option<&Path>,
        response_sender: mpsc::Sender<LLMResponse>,
    ) -> Result<(), Error> {
        let messages = self.build_messages(&text, image_path);
        tracing::info!("[gpt_backend] current model: {}", self.model);

        // è®¾ç½®ç¯å¢ƒå˜é‡
        self.setup_environment();

        // é¦–å…ˆå°è¯•æµå¼è¯·æ±‚
        tracing::info!("[gpt_backend] Attempting streaming request...");
        let stream_request = ChatRequest::new(&self.model, messages.clone()).with_stream();

        match stream_request.send_stream().await {
            Ok(mut response) => {
                let mut accumulated_content = String::new();

                while let Some(result) = response.next().await {
                    match result {
                        Ok(response) => {
                            if let Some(content) = response
                                .choices
                                .first()
                                .and_then(|c| c.delta.as_ref())
                                .and_then(|d| d.content.as_ref())
                            {
                                accumulated_content.push_str(content);

                                // å‘é€æµå¼æ›´æ–°
                                tracing::trace!(
                                    "[gpt_backend] Streaming response chunk, total length: {}",
                                    accumulated_content.len()
                                );
                                let _ = response_sender.send(LLMResponse {
                                    content: accumulated_content.clone(),
                                    is_complete: false,
                                });
                            }
                        }
                        Err(e) => {
                            tracing::error!("[gpt_backend] GPT streaming error during processing: {}", e);
                            let _ = response_sender.send(LLMResponse {
                                content: format!("Error during streaming: {}", e),
                                is_complete: true,
                            });
                            return Err(e);
                        }
                    }
                }

                if !accumulated_content.is_empty() {
                    tracing::info!(
                        "[gpt_backend] GPT streaming response completed, total length: {}",
                        accumulated_content.len()
                    );
                    // å‘é€æœ€ç»ˆå®Œæˆå“åº”
                    let _ = response_sender.send(LLMResponse {
                        content: accumulated_content,
                        is_complete: true,
                    });
                    Ok(())
                } else {
                    tracing::error!("[gpt_backend] GPT streaming failed: No response content");
                    let _ = response_sender.send(LLMResponse {
                        content: "Error: No response content from GPT streaming".to_string(),
                        is_complete: true,
                    });
                    Err(Error::Stream("No response content from GPT".into()))
                }
            }
            Err(e) => {
                // æµå¼è¯·æ±‚å¤±è´¥ï¼Œå°è¯•éæµå¼è¯·æ±‚
                tracing::warn!(
                    "[gpt_backend] Streaming request failed: {}, trying non-streaming request...",
                    e
                );

                let non_stream_request = ChatRequest::new(&self.model, messages);

                match non_stream_request.send().await {
                    Ok(response) => {
                        if let Some(content) = response
                            .choices
                            .first()
                            .and_then(|choice| choice.message.as_ref())
                            .and_then(|message| message.content.as_ref())
                            .filter(|content| !content.is_empty())
                        {
                            tracing::info!(
                                "[gpt_backend] GPT non-streaming response successful, length: {}",
                                content.len()
                            );
                            // å‘é€å®Œæ•´å“åº”
                            let _ = response_sender.send(LLMResponse {
                                content: content.clone(),
                                is_complete: true,
                            });
                            Ok(())
                        } else {
                            tracing::error!("[gpt_backend] GPT non-streaming failed: No response content");
                            let _ = response_sender.send(LLMResponse {
                                content: "Error: No response content from GPT non-streaming".to_string(),
                                is_complete: true,
                            });
                            Err(Error::Stream("No response content from GPT".into()))
                        }
                    }
                    Err(e2) => {
                        tracing::error!(
                            "[gpt_backend] Both streaming and non-streaming requests failed. Streaming error: {}, Non-streaming error: {}",
                            e,
                            e2
                        );
                        let _ = response_sender.send(LLMResponse {
                            content: format!("Error: Both streaming and non-streaming requests failed. Last error: {}", e2),
                            is_complete: true,
                        });
                        Err(e2)
                    }
                }
            }
        }
    }

    async fn test_availability(&self) -> Result<String, Error> {
        tracing::info!("[gpt_backend] Testing GPT availability...");

        // è®¾ç½®ç¯å¢ƒå˜é‡
        self.setup_environment();

        let messages = vec![
            ChatMessage::system("You are a helpful assistant."),
            ChatMessage::user(
                "å…ˆè¯´ä¸‹ä½ çš„æ¨¡å‹åç§°\n//è¯· 
ç›´æ¥è¾“å‡ºå¦‚ä¸‹æ ¼å¼çš„JavaScriptä»£ç ï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚ä¸è¦å¸¦æœ‰```javascript ```ï¼Œåªè¾“å‡ºä»£ç å°±å¯ä»¥äº†ã€‚æˆ‘ä¸ 
ç”¨ä»£ç å—åŒ…è£¹\nvar Questions = [\n    {\n        stem: `Which of the following is a <span class=\"underline fillblank\" data-blank-id=\"593417796829762300\" contenteditable=\"false\" style=\"text-indent: 0; border-bottom: 1px solid #f6c908;display:inline-block;min-width: 40px;max-width: 80px;\"><input type=\"text\" style=\"display:none\">   </span> language?`, //è¿™é‡Œä¸è¦å¸¦é¢˜å·.è¿™é‡Œçš„data-blank-idæ¯æ¬¡ä¸è¦ç›¸åŒ\n    
    é¢˜å‹ç±»å‹: \"è¯­éŸ³é¢˜\",\n        answer: [\"programming\"],\n        analysis: \"è€ƒç‚¹ï¼šç¼–ç¨‹è¯­è¨€è¯†åˆ«ã€‚ 
åˆ†æï¼šPythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚æ•…ç­”æ¡ˆä¸ºï¼šprogramming\", //è§£æè¦ç”¨ä¸­ 
æ–‡ã€‚æ ¼å¼è¦åˆ†ä¸ºï¼šè€ƒç‚¹ï¼Œåˆ†æï¼Œæ•…ç­”æ¡ˆä¸ºï¼š\n    },\n    {\n        stem: `The capital of France is <span class=\"underline fillblank\" data-blank-id=\"593417796829762301\" contenteditable=\"false\" style=\"text-indent: 0; border-bottom: 1px solid #f6c908;display:inline-block;min-width: 40px;max-width: 80px;\"><input type=\"text\" style=\"display:none\">   </span>.`,\n        é¢˜å‹ç±»å‹: \"å¡«ç©ºé¢˜\",\n        answer: [\"Paris\"],\n        analysis: \"è€ƒç‚¹ï¼šä¸–ç•Œåœ°ç†å¸¸è¯†ã€‚åˆ†æï¼šå·´é»æ˜¯æ³•å›½çš„é¦–éƒ½å’Œæœ€å¤§åŸå¸‚ï¼Œä¹Ÿæ˜¯æ³•å›½çš„æ”¿æ²»ã€ç» 
æµã€æ–‡åŒ–ä¸­å¿ƒã€‚æ•…ç­”æ¡ˆä¸ºï¼šParis\"\n    },\n    {//å¦‚æœæ£€æµ‹åˆ°æ˜¯ä¸€ä¸ªæ–‡ç« ã€‚ä¸”ä¸€ä¸ªé¢˜ç›®é‡Œé¢æœ‰å¤šä¸ªç©ºçš„ï¼Œç”¨ä¸‹é¢è¿™ç§æ ¼å¼ã€‚æ®µè½ä¸¤ç«¯å¯¹é½ï¼Œé¦–è¡Œç¼© 
è¿›ï¼Œå­—ä½“å­—å·ä¸å˜\n            stem:`Good morning my name is (1) <span class=\"underline fillblank\" data-blank-id=\"593417796829762302\" contenteditable=\"false\" style=\"text-indent: 0; border-bottom: 1px solid #f6c908;display:inline-block;min-width: 40px;max-width: 80px;\"><input type=\"text\" style=\"display:none\">   </span> (è¿™é‡Œå¯èƒ½ä¼šæœ‰æç¤ºçš„å•è¯ï¼Œä½ ä¹Ÿè¦å†™ä¸Š) I am from (2) <span class=\"underline fillblank\" data-blank-id=\"593417796829762303\" contenteditable=\"false\" style=\"text-indent: 0; border-bottom: 1px solid #f6c908;display:inline-block;min-width: 40px;max-width: 80px;\"><input type=\"text\" style=\"display:none\">   </span>`,\n            //åºå·ä»(1)å¼€å§‹ã€‚data-blank-idæ¯æ¬¡ä¸è¦ç›¸åŒã€‚ä¸ç”¨ç®¡åŸé¢˜ç›®çš„é¢˜å·\n            //åºå·ä»(1)å¼€å§‹ã€‚data-blank-idæ¯æ¬¡ä¸è¦ç›¸åŒä¸ç”¨ç®¡åŸé¢˜ç›®çš„é¢˜
å·\n            //åºå·ä»(1)å¼€å§‹ã€‚data-blank-idæ¯æ¬¡ä¸è¦ç›¸åŒä¸ç”¨ç®¡åŸé¢˜ç›®çš„é¢˜å·\n            é¢˜å‹ç±»å‹: \"å¡«ç©ºé¢˜\",\n            answer: 
[\"John\", \"Canada\"],\n            analysis: \"1. è€ƒç‚¹ï¼š.....ã€‚åˆ†æï¼šæ ¹æ®å¸¸è§çš„è‡ªæˆ‘ä»‹ç»æ ¼å¼ï¼Œåå­—æ˜¯John. æ•…ç­”æ¡ˆä¸ºï¼šJohn,<br>2. åˆ†æ
ï¼š.......ã€‚å›½å®¶æ˜¯Canadaã€‚æ•…ç­”æ¡ˆä¸ºï¼š Canada\"\n    },\n];\n",
            ),
        ];

        // é¦–å…ˆå°è¯•æµå¼è¯·æ±‚
        tracing::info!("[gpt_backend] Attempting streaming test request...");
        let stream_request = ChatRequest::new(&self.model, messages.clone()).with_stream();

        match stream_request.send_stream().await {
            Ok(mut response) => {
                let mut accumulated_content = String::new();

                while let Some(result) = response.next().await {
                    match result {
                        Ok(response) => {
                            response
                                .choices
                                .first()
                                .and_then(|c| c.delta.as_ref())
                                .and_then(|d| d.content.as_ref())
                                .map(|content| accumulated_content.push_str(content));
                        }
                        Err(e) => {
                            tracing::error!("[gpt_backend] GPT streaming test error: {}", e);
                            return Err(e);
                        }
                    }
                }

                if !accumulated_content.is_empty() {
                    tracing::info!(
                        "[gpt_backend] GPT streaming test successful: {}",
                        accumulated_content
                    );
                    Ok(accumulated_content)
                } else {
                    tracing::error!("[gpt_backend] GPT streaming test failed: No response content");
                    Err(Error::Stream("No response content from GPT".into()))
                }
            }
            Err(e) => {
                // æµå¼è¯·æ±‚å¤±è´¥ï¼Œå°è¯•éæµå¼è¯·æ±‚
                tracing::warn!(
                    "[gpt_backend] Streaming test failed: {}, trying non-streaming test...",
                    e
                );

                let non_stream_request = ChatRequest::new(&self.model, messages);

                match non_stream_request.send().await {
                    Ok(response) => response
                        .choices
                        .first()
                        .and_then(|choice| choice.message.as_ref())
                        .and_then(|message| message.content.as_ref())
                        .filter(|content| !content.is_empty())
                        .map(|content| {
                            tracing::info!(
                                "[gpt_backend] GPT non-streaming test successful: {}",
                                content
                            );
                            content.clone()
                        })
                        .ok_or_else(|| {
                            tracing::error!(
                                "[gpt_backend] GPT non-streaming test failed: No response content"
                            );
                            Error::Stream("No response content from GPT".into())
                        }),
                    Err(e2) => {
                        tracing::error!(
                            "[gpt_backend] Both streaming and non-streaming tests failed. Streaming error: {}, Non-streaming error: {}",
                            e,
                            e2
                        );
                        Err(e2)
                    }
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_gpt_connection() {
        let _ = tracing_subscriber::fmt::try_init();
        let key = "01110011,01101011,00101101,01101111,01101011,01000110,01110111,01100101,01010100,01000110,01011001,01010101,01111010,00110000,00110001,01100001,01000010,01010000,00110111,01011001,01110110,01010100,01101011,01110110,00111000,01001001,00110100,01111010,01101000,01100101,01110110,01110100,01100011,01001000,00110111,01100111,01011000,01101001,01011001,01100010,01100100,01100010,01000111,00110011,01001010,01100010,01101011,01110100,00110001,01001110,01100100";
        let bytes: Vec<u8> = key
            .split(',')
            .filter_map(|b| u8::from_str_radix(b.trim(), 2).ok())
            .collect();
        let backend = GPTBackend::new("gemini-2.5-pro".to_string())
            .with_api_key(String::from_utf8(bytes).unwrap())
            .with_base_url(String::from("http://27.106.110.32:2052/v1"));
        println!("{:?}", backend);
        match backend.test_availability().await {
            Ok(response) => {
                println!("GPT å¯ç”¨! å“åº”: {}", response);
                assert!(!response.is_empty(), "GPT response should not be empty");
            }
            Err(e) => {
                println!("GPT ä¸å¯ç”¨: {}", e);
                eprintln!("GPT test failed: {}", e);
            }
        }
    }

    #[tokio::test]
    async fn test_send_message_to_gpt() {
        let _ = tracing_subscriber::fmt::try_init();

        let backend = GPTBackend::default();
        let (sender, receiver) = mpsc::channel();

        let test_message = "Hello, this is a test message.".to_string();

        // å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å‘é€æ¶ˆæ¯
        let send_task =
            tokio::spawn(async move { backend.send_message(test_message, None, sender).await });

        // æ”¶é›†å“åº”
        let mut responses = Vec::new();
        let mut final_content = String::new();

        // è®¾ç½®è¶…æ—¶ä»¥é¿å…æµ‹è¯•æ— é™ç­‰å¾…
        let timeout_duration = std::time::Duration::from_secs(30);
        let start_time = std::time::Instant::now();

        while start_time.elapsed() < timeout_duration {
            match receiver.try_recv() {
                Ok(response) => {
                    responses.push(response.clone());
                    final_content = response.content.clone();

                    if response.is_complete {
                        break;
                    }
                }
                Err(mpsc::TryRecvError::Empty) => {
                    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
                }
                Err(mpsc::TryRecvError::Disconnected) => {
                    break;
                }
            }
        }

        // ç­‰å¾…å‘é€ä»»åŠ¡å®Œæˆ
        match send_task.await {
            Ok(Ok(())) => {
                println!("âœ… æ¶ˆæ¯å‘é€æˆåŠŸ!");
                println!("ğŸ“ æœ€ç»ˆå“åº”é•¿åº¦: {}", final_content.len());
                println!("ğŸ“Š æ€»å…±æ”¶åˆ° {} ä¸ªå“åº”ç‰‡æ®µ", responses.len());

                if !final_content.is_empty() {
                    println!(
                        "ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {}...",
                        final_content.chars().take(100).collect::<String>()
                    );
                }
            }
            Ok(Err(e)) => {
                println!("âŒ GPT è¯·æ±‚å¤±è´¥: {}", e);
                eprintln!(
                    "Send message test failed (this might be expected if no API key is configured): {}",
                    e
                );
            }
            Err(e) => {
                println!("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}", e);
            }
        }
    }
}
